{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "def initialize_db(db_name=\"context_DPM_data.db\"):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create a table to store contexts\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS contexts\n",
    "                     (id INTEGER PRIMARY KEY, context TEXT)''')\n",
    "    conn.commit()\n",
    "    return conn, cursor\n",
    "\n",
    "# Connect to DB\n",
    "conn, cursor = initialize_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "\n",
    "# Load the fine-tuned BERT model and tokenizer for Question Answering\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_answer_and_confidence(context, question):\n",
    "    # Tokenize input and get output from the model\n",
    "    inputs = tokenizer(question, context, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Get start and end scores for answer\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    # Find the best start and end token positions\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1  # +1 because end token is inclusive\n",
    "\n",
    "    # Convert token IDs to string\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
    "\n",
    "    # Calculate confidence score (using softmax to get probability distribution)\n",
    "    start_probs = torch.nn.functional.softmax(answer_start_scores, dim=-1)\n",
    "    end_probs = torch.nn.functional.softmax(answer_end_scores, dim=-1)\n",
    "\n",
    "    confidence = (start_probs.max().item() + end_probs.max().item()) / 2\n",
    "\n",
    "    return answer, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def rank_contexts_by_relevance(question, contexts, top_n=5):\n",
    "    # Vectorize the question and contexts using TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer().fit_transform([question] + contexts)\n",
    "    \n",
    "    # Compute the cosine similarity between the question and each context\n",
    "    cosine_similarities = cosine_similarity(tfidf_vectorizer[0:1], tfidf_vectorizer).flatten()\n",
    "    \n",
    "    # Get the indices of the top_n most similar contexts\n",
    "    relevant_indices = cosine_similarities.argsort()[:-top_n-1:-1]\n",
    "    \n",
    "    # Select the most relevant contexts based on the indices\n",
    "    most_relevant_contexts = [contexts[i-1] for i in relevant_indices][1:]  # We exclude the first item since it's the question itself\n",
    "\n",
    "    return list(set(most_relevant_contexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contexts_for_filter_keywords(conn, cursor, question, filter_keywords):\n",
    "    \n",
    "    # Construct the query to search for contexts containing keywords\n",
    "    query_clauses = [\"context LIKE ?\" for _ in filter_keywords]\n",
    "    query = \"SELECT context FROM contexts WHERE \" + \" OR \".join(query_clauses)\n",
    "    params = ['%' + keyword + '%' for keyword in filter_keywords]\n",
    "    \n",
    "    cursor.execute(query, params)\n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    # Extract contexts from the results\n",
    "    contexts = [result[0] for result in results]\n",
    "    relevant_contexts = rank_contexts_by_relevance(question, contexts)\n",
    "    \n",
    "    return relevant_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def filter_keywords(question):\n",
    "    keywords = question.split()\n",
    "    filtered_keywords = [word for word in keywords if word.lower() not in stop_words and len(word) > 2]  # We also filter out words with length <= 2\n",
    "    return filtered_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is termed as Liquidated Damages?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"What are types of Liquidated Damages?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"What is Consequential Damages?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"What is Force Majeure?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"Whare is the Alternative remedies to Risk & Expense Purchase Clause?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Answer with Confidence score printer\n",
    "def get_answers_for_all_contexts(question):\n",
    "    # Get contexts related to the question from your database\n",
    "    filtered_keywords = filter_keywords(question)\n",
    "    contexts = get_contexts_for_filter_keywords(conn, cursor, question, filtered_keywords)\n",
    "    best_confidence = -1\n",
    "    best_answer = \"\"\n",
    "    # Go through each context and generate answers\n",
    "    for context in contexts:\n",
    "        answer, confidence = get_answer_and_confidence(context, question)\n",
    "        if confidence > best_confidence:\n",
    "            best_confidence = confidence\n",
    "            best_answer = answer\n",
    "        print(f\"Context: {context}\")\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(f\"Confidence Score: {confidence:.4f}\")\n",
    "        print(\"-\" * 50)  # separator line for better readability\n",
    "    print(\"#\"*50)\n",
    "    print(\"The best answer is: \", best_answer)\n",
    "\n",
    "get_answers_for_all_contexts(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
